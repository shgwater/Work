---
title: 2020-3-4 Hadoop学习
tags: 新建,模板,小书匠
grammar_cjkRuby: true
---


# 创建虚拟机
## 用户名及密码

- root
- rootroot

## hadoop环境配置

### 安装系统及hadoop

依据博客进行  https://www.cnblogs.com/xzjf/p/7231519.html

### 遇到的问题

#### 使用2.6.5的hadoop安装包时报错。
``` 
20/03/10 01:04:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
```

解决办法，重新下载2.7.7的安装包，重新安装之后可以正常使用，但是还是报错。但是此时不营销使用。

2020年3月11日 更新
原因：
Apache提供的hadoop本地库是32位的，而在64位的服务器上就会有问题，因此需要自己编译64位的版本。

1、首先找到对应自己hadoop版本的64位的lib包，可以自己手动去编译，但比较麻烦，也可以去网上找，好多都有已经编译好了的。

2、可以去网站：http://dl.bintray.com/sequenceiq/sequenceiq-bin/  下载对应的编译版本

3、将准备好的64位的lib包解压到已经安装好的hadoop安装目录的lib/native 和 lib目录下：

		[hadoop@hadoopTest ~]$ tar -xvf hadoop-native-64-2.7.0.tar -C hadoop-2.7.2/lib/native
		[hadoop@hadoopTest ~]$ tar -xvf hadoop-native-64-2.7.0.tar -C hadoop-2.7.2/lib
		
4、然后增加环境变量：

		[hadoop@hadoopTest hadoop-2.7.2]$ vi /etc/profile
		
5、增加下面的内容：

		export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
		export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
6、让环境变量生效

		[hadoop@hadoopTest hadoop-2.7.2]$ source /etc/profile
7、自检hadoop checknative –a 指令检查

		[hadoop@hadoopTest hadoop-2.7.2]$ hadoop checknative –a

#### 启动后 Master 没有 Datanode

1、查看 hadoop/etc/hadoop/slaves 文件中的配置是否正确。
2、根据日志中的路径，cd /home/storm/hadoop/tmp/dfs，能看到 data和name两个文件夹， 将name/current下的VERSION中的clusterID复制到data/current下的VERSION中，覆盖掉原来的clusterID。 让两个保持一致，然后重启，启动后执行jps，查看进程： 20131 SecondaryNameNode 20449 NodeManager 19776 NameNode 21123 Jps 19918 DataNode 20305 ResourceManager
（出现该问题的原因:
在第一次格式化dfs后，启动并使用了hadoop，后来又重新执行了格式化命令（hdfs namenode -format)，这时namenode的clusterID会重新生成，而datanode的clusterID 保持不变。）

### 安装2.7.7 之后启动报错

 core-site.xml:1:1: Content is not allowed in prolog
 
 ### 安装完成之后启动
 
 [hadoop@Master hadoop]$ ./sbin/start-dfs.sh
[hadoop@Master hadoop]$ ./sbin/start-yarn.sh

启动之后再启动hive。
 
 ## hadoop 的一些认识。
 
 ### hdfs 是什么
 
 HDFS 是 Hadoop 的分布式文件系统的简称。
 在 HDFS 目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default 数据库，直接在数据仓库目录下创建一个文件夹。
 与linux自带的文件系统不同的另外一套文件系统，需要使用程序读取。
 
 ### hdfs 常用操作
 
 创建 .profile  内容如下
 ``` python
 #Hadoop
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-DJava.library.path=$HADOOP_HOME/lib"
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH

 ```
 之后 source .profile 既配置好了全局变量，就不需要每次都 进入到 /usr/local/hadoop/ 中 ./bin/hadoop 来调用命令了，直接在 hadoop 用户下  hadoop即可。
 ``` python
 # 查看 hdfs 的目录结构
 
[hadoop@Master hadoop]$ ./bin/hdfs dfs -ls /user/hive/
[hadoop@Master hadoop]$ ./bin/hadoop fs -ls /

# 递归结构显示文件目录 -ls -R

[hadoop@Master hadoop]$ ./bin/hadoop fs -ls -R /

# 创建目录

[hadoop@Master hadoop]$ ./bin/hadoop fs -mkdir /user/hive/test

# hadoop fs -rm   删除文件，-rm -R 递归删除目录和文件

 [hadoop@Master hadoop]$ ./bin/hadoop fs -rm -R /user/hive/test
 
 # hadoop fs -put  [localsrc] [dst]  从本地加载文件到HDFS
 
 [hadoop@Master hadoop]$ ./bin/hadoop fs -put test.txt /user/hive/test
 
	 # 加载/opt/module/datas/student.txt 文件到student数据库表中。
 
	hive> load data local inpath '/opt/module/datas/student.txt' into table student;
	
# hadoop fs -get  [dst] [localsrc]  从HDFS导出文件到本地

 [hadoop@Master hadoop]$ ./bin/hadoop fs -get /user/hive/test/test.txt

# hadoop fs -text  查看文件内容

[hadoop@Master hadoop]$ hadoop fs -text /user/hive/warehouse/student/student.txt

# hadoop fs -du  统计目录下各文件大小，单位字节。-du -s 汇总目录下文件大小，-du -h 显示单位

[hadoop@Master ~]$ hadoop fs -du -s -h /

 
 ```
 
 ## hive
 
 ### 安装hive
 
 见 大数据技术之HIVE.doc 。
 
 ### hive的基本操作
  
``` python

# 1. 启动hive
[root@hadoop102 hive]$ bin/hive
# 2. 查看数据库
hive>show databases;
# 3. 打开默认数据库
hive>use default;
# 4. 显示default数据库中的表
hive>show tables;
# 5. 创建一张表
hive> create table student(id int, name string) ;
# 6. 显示数据库中有几张表
hive>show tables;
# 7. 查看表的结构
hive>desc student;
# 8. 向表中插入数据
hive> insert into student values(1000,"ss");
# 9. 查询表中数据
hive> select * from student;
# 1. ）退出hive
hive> quit;

```

### 将本地文件导入 Hive 案例

- 需求 将本地目录下的txt文件导入hive中的表中。结构为student(id int, name string)

1. 创建测试txt数据，注意两列数据中间的分隔符为 tab。
2. 在hive中创建表结构。
``` python
show databases;
use default;
show tables;
desc student;
# 创建表并且声明文件分隔符 '\t' 。
create table student(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
# 加载txt数据到表中
load data local inpath '/home/hadoop/datas/student.txt' into table student;

select * from student;

```
